# Inter-Screener

Inter-Screener is a project designed to analyze and evaluate various aspects of communication skills by processing user-provided videos. It incorporates a combination of open-source libraries and models to assess speech rate, tone of voice, rhythm, confidence, spectrogram, engagement, body language, and more. The project aims to assist students in improving their communication abilities.

## Disclaimer

Inter-Screener is an open-source project provided as-is without any warranty or guarantee. The evaluations and assessments generated by the application are based on models and algorithms, and their accuracy may vary. Use the results and insights provided by Inter-Screener as guidelines, and exercise your judgment when interpreting them.

## Features

Inter-Screener currently offers the following features:

[x] Speech Rate: Analyzes the speed at which the user speaks.
[x] Tone of Voice: Assesses the tone and emotional quality of the user's voice.
[x] Rhythm: Evaluates the rhythmic patterns in the user's speech.
[x] Confidence: Measures the level of confidence exhibited by the user.
[x] Spectrogram: Generates a visual representation of the audio frequency content.
[x] Engagement: Utilizes head and other movements to determine the user's level of engagement.
[ ] Body Language (upcoming): Analyzes the user's body language to assess posture, gestures, and energy level.
[ ] Eye Contact (upcoming): Evaluates the user's eye contact with the camera.
[ ] Verbal Fluency (upcoming): Assesses the user's ability to express thoughts smoothly and effectively.
[ ] Posture (upcoming): Analyzes the user's body posture during communication.
[ ] Gestures (upcoming): Evaluates the use of hand gestures during the video.
[ ] Energy Level (upcoming): Measures the level of energy exhibited by the user.
[ ] Use of Filler Words (upcoming): Identifies the frequency and usage of filler words.
[ ] Pacing (upcoming): Analyzes the user's speed and rhythm of speech.
[ ] Nervous Habits (upcoming): Identifies and evaluates nervous habits exhibited by the user.
[ ] Vocal Pitch (upcoming): Assesses the pitch and tonal variations in the user's voice.
[ ] Confidence Indicators (upcoming): Identifies specific indicators of confidence in the user's behavior.
[ ] Confidence Fluctuations (upcoming): Tracks and analyzes variations in the user's confidence levels.

## Installation

To set up Inter-Screener, follow these steps:

- Clone the repository to your local machine:

```bash
git clone https://github.com/basicfunc/inter-screener
```

- Navigate to the project directory:

```bash
cd inter-screener
```

- Install the required dependencies:
```
pip install -r requirements.txt
```

- Rename `example.env` to `.env`

- Update the necessary configuration in the `.env` file by specifing the paths for the various models and libraries required for analysis.

## Contributing

Contributions to Inter-Screener are welcome! If you would like to contribute to the project, please follow these steps:

- Fork the repository.

- Create a new branch for your feature or bug fix:
```bash
git checkout -b feature/your-feature-name
```

- Make your changes and commit them with descriptive messages:
```
git commit -m "Add feature/bug fix description"
```

- Push your changes to your forked repository:
```
    git push origin feature/your-feature-name
```

- Open a pull request in the main repository.

## License

Inter-Screener is licensed under the MIT License. You can find the details in the LICENSE file.

## Acknowledgments

Inter-Screener relies on the following open-source libraries, models, and resources:

- Librosa: https://github.com/librosa/librosa
- TensorFlow: https://github.com/tensorflow/tensorflow
- OpenCV: https://github.com/opencv/opencv
- OpenAI's Whisper Model: https://github.com/openai/whisper
- Whisper.cpp: https://github.com/ggerganov/whisper.cpp
- GPT-j: https://github.com/kingoflolz/mesh-transformer-jax
- gpt4all: https://github.com/nomic-ai/gpt4all
- LangChain: https://github.com/hwchase17/langchain

I express my gratitude to the developers and contributors of these projects for making their work publicly available.